{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 1, Asymmetric loss regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data and plot the training / val data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load data\n",
    "dataTrain=np.loadtxt('housing-train.txt')\n",
    "dataVal=np.loadtxt('housing-val.txt')\n",
    "\n",
    "X_train=(dataTrain[:,np.newaxis,0])\n",
    "y_train=dataTrain[:,1]\n",
    "X_val=dataVal[:,np.newaxis,0]\n",
    "y_val=dataVal[:,1]\n",
    "\n",
    "plt.plot(X_train, y_train, 'k.')\n",
    "plt.plot(X_val, y_val, 'b.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will solve the linear regression (under the \"normal\" symmetric squared loss) using the closed form solution. Let's define a couple of functions we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def symmLoss(X, w ,y):\n",
    "    \"\"\"\n",
    "    Get the symmetric squared loss given data X, weight w and ground truth y\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        N x d+1 data matrix (row per example)\n",
    "    w : 1D array\n",
    "        d+1 length vector\n",
    "    y : 1D array\n",
    "        Observed function values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : a scalar\n",
    "        The loss calculated by the symmetric loss formula\n",
    "    \"\"\"\n",
    "    \n",
    "    return   *********** expression for calculating the loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the loss we have defined is the normalized squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lsqClosedForm(X, y):\n",
    "    \"\"\"\n",
    "    Use closed form solution for least squares minimization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        N x d+1 data matrix (row per example)\n",
    "    y : 1D array\n",
    "        Observed function values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w : 1D array\n",
    "        d+1 length vector\n",
    "    \"\"\"\n",
    "    return np.dot(np.linalg.pinv(X), y) # lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the closed form solution: generate a toy data set from a random linear function with <b>no noise</b>. We should be able to perfectly recover w in this case (up to numerical precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones([20,1]),np.random.random((20,1))))\n",
    "w = np.random.random((2))\n",
    "y = np.dot(X,w)\n",
    "print('true weight:  '+repr(w))\n",
    "w_ = lsqClosedForm(X, y)\n",
    "print('function output: '+repr(w_))\n",
    "if (np.allclose(w,w_)):\n",
    "    print('Close enough')\n",
    "else:\n",
    "    print('Uh-oh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to estimate the variance of the noise and the log likelihood of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logLikelihood(X, w, y):\n",
    "    \"\"\"\n",
    "    Get the estimated variance, and the log likelihood of the data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        N x d+1 design matrix (row per example)\n",
    "    w : 1D array\n",
    "        d+1 length vector\n",
    "    y : 1D array\n",
    "        Observed function values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    simga2 : a scalar\n",
    "        The estimated variance (sigma squared)\n",
    "    loglike : a scalar\n",
    "        The log-likelihood under the Gaussian noise model N(0,sigma2)\n",
    "    \"\"\"\n",
    "    N = X.shape[0]   # number of rows in X\n",
    "    \n",
    "    # now estimate the variance of the Gaussian noise\n",
    "    sigma2 = **************** your code for estimating noise variance here\n",
    "    \n",
    "    # normalized log-likelihood (mean of per-data point log-likelihood of the model given by w,sigma2)\n",
    "    loglike = ******************** your code for computing normalized (i.e., average) log-likelihood of the model w on X,y\n",
    "    \n",
    "    return sigma2, loglike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit linear, quadratic and cubic models to the training data, and plot the fit function. First we need to define the function that will map the original input to an expanded feature vector (including the constant term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def degexpand(X, deg, C=None):\n",
    "    \"\"\"\n",
    "    Prepares data matrix with a column of ones and polynomials of specified\n",
    "    degree.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        n x d data matrix (row per example)\n",
    "    deg : integer\n",
    "        Degree of polynomial\n",
    "    C : 1D array\n",
    "        Scaling weights. If not specifed, weights are calculated to fit each\n",
    "        columns of X in [-1, 1].\n",
    "        Note: It is shown in problem set 1 that this normalization does\n",
    "        not affect linear regression, as long as it is applied\n",
    "        consistently to training *and* test data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_X : 2D array\n",
    "        n x ((deg*d) + 1) data matrix (row per example)\n",
    "        *******Fill the comments assuming d = 1*******\n",
    "        The output is arranged as follows:\n",
    "            - X[:, 0] is all ones \n",
    "            - X[:, 1] is FILL HERE\n",
    "                .\n",
    "                .\n",
    "                .\n",
    "            - X[:,-2] is FILL HERE\n",
    "            - X[:,-1] is FILL HERE\n",
    "    C : 1D array\n",
    "        Scaling weights that were used. Can be used in scaling other data later in the same way.\n",
    "    \"\"\"\n",
    "    ******** Provide comments where ever asked for (this is the only place where you are asked to fill comments) ****************\n",
    "    assert X.ndim == 2\n",
    "    n, m = X.shape\n",
    "\n",
    "    # Make polynomials\n",
    "    out_X = (X[..., np.newaxis] ** (1. + np.arange(deg))).reshape(n, -1)\n",
    "\n",
    "    # Add column of ones\n",
    "    out_X = np.concatenate([np.ones((out_X.shape[0], 1)), out_X], axis=1)\n",
    "\n",
    "    if C is None:\n",
    "        C = abs(out_X).max(axis=0)\n",
    "    else:\n",
    "        assert np.shape(C) == (out_X.shape[1],), \"C must match outgoing matrix\"\n",
    "\n",
    "    out_X /= C # divide dimension-wise\n",
    "    return out_X, C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit models to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = np.Inf\n",
    "plt.plot(X_train, y_train, 'k.')\n",
    "\n",
    "# Try different polynomial degrees (feel free to add additional values to explore)\n",
    "for deg in [1,2,3,5]:\n",
    "    # Expand data first; you can check how this function works in utils.py\n",
    "    X, C = degexpand(X_train, deg)\n",
    "    y = y_train\n",
    "    \n",
    "    # Get the result by applying closed-form equation\n",
    "    w = lsqClosedForm(X, y)\n",
    "    \n",
    "    # compute loss on training set\n",
    "    loss = symmLoss(X, w, y)\n",
    "    \n",
    "    # compute loss on val set; note -- use the same scaling matrix C as for training\n",
    "    val_loss = symmLoss(degexpand(X_val, deg, C)[0], w, y_val)\n",
    "    \n",
    "    print('degree %d:' %(deg))\n",
    "    print('train loss %.6f' %(loss))\n",
    "    print('val loss %.6f' %(val_loss))\n",
    "    print('sigma^2: %.6f \\nlog-likelihood %.6f\\n' %logLikelihood(X, w, y))\n",
    "    \n",
    "    ********************** insert code to keep track of the best model so far\n",
    "    \n",
    "    # write the code above so that best_param[0] = w of the best model, \n",
    "    #                              best_param[1]=deg of the best model, \n",
    "    #                              best_param[2]=C (scaling matrix) of the best model\n",
    "    \n",
    "    # Plot the function\n",
    "    color = {1:'b', 2:'g', 3:'r', 5:'c'}[deg]\n",
    "    plt.plot(np.linspace(min(X_train)-.1,max(X_train)+.1), np.dot(degexpand(np.linspace(min(X_train)-.1,max(X_train)+.1).reshape((50, 1)), deg, C)[0], w), color,label='deg'+str(deg))\n",
    "\n",
    "\n",
    "                             \n",
    "    \n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,40])\n",
    "axes.set_ylim([0,55])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Announce result of best selected model on validation data\n",
    "\n",
    "dataTest=np.loadtxt('housing-val.txt')\n",
    "print(\"Best degree:\"+repr(best_param[1])+\", loss (sym) on val:\")\n",
    "print(symmLoss(degexpand(X_val, best_param[1], best_param[2])[0], best_param[0], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(explain your logic for selecting the model)\n",
    "\n",
    "Now we want to repeat the experiment above but under the asymmetric loss function. Since there is no closed form solution, we will need to rely on gradient descent. First we need to implement the loss function, including the option of calculating the gradient of loss with respect to model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymmLoss(X, w, y,alpha,_grad=False):\n",
    "    \"\"\"\n",
    "    Get the asymmetric loss given data X, weight w and ground truth y\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        N x d+1 design matrix (row per example)\n",
    "    w : 1D array\n",
    "        d+1 length vector\n",
    "    y : 1D array\n",
    "        Observed function values\n",
    "    alpha : scalar\n",
    "        indicating multiplicative factor on underprediction loss (when prediction is lower than ground truth)\n",
    "    _grad : boolean\n",
    "        if True, compute and return gradient of the loss w.r.t. w on the data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : a scalar\n",
    "        The loss calculated by equation in problem set 1\n",
    "    grad : 1D array  (only when _grad == True)\n",
    "        vector of same dimension as w, containing the gradient of loss w.r.t. w\n",
    "    \"\"\"\n",
    "       \n",
    "    ********** enter your code -- you should be able to largely replicate code in symmLoss, \n",
    "    ********** with some modifications to account for alpha\n",
    "\n",
    "\n",
    "    if _grad:\n",
    "        grad = *************** calculation for gradient\n",
    "        return loss, grad\n",
    "    else:\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advice: It might beneficial to test the loss and gradient function using cooked up X, w, y, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have had the functions to calculate loss and gradient, we can implement the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradDescent(X, y,alpha,tol=1e-4,maxIt=10000,checkit=1000,verbose=2,lr=0.1):\n",
    "    \"\"\"\n",
    "    Use gradient descent to min(loss(X, w, y))\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        N x d+1 design matrix (row per example)\n",
    "    y : 1D array\n",
    "        Observed function values\n",
    "    alpha : scalar\n",
    "        weight on positive errors in asymmetric loss\n",
    "    tol : scalar\n",
    "        tolerance on loss decrease (default 1e-4)\n",
    "    maxIt : scalar\n",
    "        max allowed number of iterations (default 10000)\n",
    "    checkit : scalar\n",
    "        # of iterations after which the progress is checked for convergence\n",
    "    verbose : scalar\n",
    "        0  -- no progress report; 1 -- every checkit*10; 2 -- every checkit (default 2)\n",
    "    lr : scalar\n",
    "        learning rate (default 0.1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w : 1D array\n",
    "        d+1 length vector\n",
    "        \n",
    "    it: number of iterations until convergence\n",
    "    \"\"\"\n",
    "    # Random initialize the weight\n",
    "    w = np.random.randn(X.shape[1])\n",
    "    it = 0\n",
    "    lastloss = np.Inf\n",
    "    while True:\n",
    "        it += 1 # advance iteration count\n",
    "        loss, grad = asymmLoss(X, w, y,alpha,_grad=True) # evaluate loss and compute gradient\n",
    "        \n",
    "        if it % checkit == 0:\n",
    "            converged = it >= maxIt or loss > lastloss-tol\n",
    "            lastloss = loss\n",
    "            if verbose==2 or verbose==1 and it % (10*checkit) == 0:\n",
    "                print('iter %d:  loss %.4f' %(it,loss))\n",
    "            if converged:\n",
    "                break\n",
    "            \n",
    "        w = **************** add code here # update w (only if continuing updates)\n",
    "       \n",
    "        \n",
    "    return w, it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test gradient descent using any alpha and data generated by (random) noiseless linear model; we should recover the true w accurately (although possibly with less accuracy than the closed form solution for alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones([20,1]),np.random.random((20,1))))\n",
    "w = np.random.random((2))\n",
    "y = np.dot(X,w)\n",
    "print('true weight:'+repr(w))\n",
    "w_, it_ = gradDescent(X, y,10,maxIt=10000,tol=1e-6)\n",
    "print('%d iterations' %it_)\n",
    "print('function output:'+repr(w_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start testing the preformance of models using aymmetric loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_loss = np.Inf\n",
    "\n",
    "plt.plot(X_train, y_train, '.')\n",
    "\n",
    "\n",
    "for deg in[1,2,3,5]:\n",
    "    # Expand data first\n",
    "    X, C = degexpand(X_train, deg)\n",
    "    y = y_train\n",
    "        \n",
    "    # Run gradient descent\n",
    "    w, _ = gradDescent(X, y,alpha,maxIt=50000,checkit=1000,verbose=1,lr=1)\n",
    "    loss = asymmLoss(X, w, y,alpha)\n",
    "    val_loss = asymmLoss(degexpand(X_val, deg, C)[0], w, y_val,alpha)\n",
    "    \n",
    "    print('degree %d:' %(deg))\n",
    "    print('train loss %.6f' %(loss))\n",
    "    print('val loss %.6f\\n' %(val_loss))\n",
    "    \n",
    "    ************** code for selecting best model\n",
    "    # your code should store best_param_asym in the same format as for the symmetric loss experiments above\n",
    "        \n",
    "    # Plot the function\n",
    "    color = {1:'b', 2:'g', 3:'r', 5:'c'}[deg]\n",
    "    plt.plot(np.linspace(min(X_train)-.1,max(X_train)+.1), np.dot(degexpand(np.linspace(min(X_train)-.1,max(X_train)+.1).reshape((50, 1)), deg, C)[0], w), color,label='deg'+str(deg))\n",
    "    \n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,40])\n",
    "axes.set_ylim([0,55])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Announce result on validation data\n",
    "\n",
    "dataTest=np.loadtxt('housing-val.txt')\n",
    "print(\"Best degree:\"+repr(best_param[1])+\", loss (asym) on val:\")\n",
    "print(asymmLoss(degexpand(X_val, best_param_asym[1], best_param_asym[2])[0], best_param_asym[0], y_val,alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now measure results on test with asymmetric loss, for both models (one learned with sym loss and one learned with asym loss)\n",
    "\n",
    "dataTest=np.loadtxt('housing-test.txt')\n",
    "X_test=(dataTest[:,np.newaxis,0])\n",
    "y_test=dataTest[:,1]\n",
    "\n",
    "print(\"Best degree (sym):\"+repr(best_param[1]))\n",
    "alpha = ******* #your alpha when over-estimating is 5 times worse \n",
    "print('Sym loss: %.4f' %symmLoss(degexpand(X_test, best_param[1], best_param[2])[0], best_param[0], y_test))\n",
    "alpha = ******* #your alpha when over-estimating is 20 times worse\n",
    "print('Asym loss: %.4f' %asymmLoss(degexpand(X_test, best_param[1], best_param[2])[0], best_param[0], y_test,alpha))\n",
    "print('Log-likelihood (symmetric): %.4f\\n' %logLikelihood(degexpand(X_test, best_param[1], best_param[2])[0], best_param[0], y_test)[1])\n",
    "\n",
    " \n",
    "\n",
    "print(\"Best degree (asym):\"+repr(best_param_asym[1]))\n",
    "print('Sym loss: %.4f' %symmLoss(degexpand(X_test, best_param_asym[1], best_param_asym[2])[0], best_param_asym[0], y_test))\n",
    "alpha = ******* #your alpha when over-estimating is 5 times worse\n",
    "print('Asym loss: %.4f' %asymmLoss(degexpand(X_test, best_param_asym[1], best_param_asym[2])[0], best_param_asym[0], y_test,alpha))\n",
    "alpha = ******* #your alpha when over-estimating is 20 times worse\n",
    "print('Asym loss: %.4f' %asymmLoss(degexpand(X_test, best_param_asym[1], best_param_asym[2])[0], best_param_asym[0], y_test,alpha))\n",
    "print('Log-likelihood (symmetric): %.4f\\n' %logLikelihood(degexpand(X_test, best_param_asym[1], best_param_asym[2])[0], best_param_asym[0], y_test)[1])\n",
    "\n",
    "\n",
    "'''\n",
    "print(\"Best degree (asym):\"+repr(best_param_asym[1]))\n",
    "print('Asym loss: %.4f' %asymmLoss(degexpand(X_test, best_param_asym[1], best_param_asym[2])[0], best_param_asym[0], y_test,alpha))\n",
    "print('Sym loss: %.4f' %symmLoss(degexpand(X_test, best_param_asym[1], best_param_asym[2])[0], best_param_asym[0], y_test))\n",
    "print('Log-likelihood (symmetric): %.4f\\n' %logLikelihood(degexpand(X_test, best_param_asym[1], best_param_asym[2])[0], best_param_asym[0], y_test)[1])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done with the experiments. Here you should write clearly and succinctly what your conclusions are: what model is better? Why? In what sense? See problem set for more instructions."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
