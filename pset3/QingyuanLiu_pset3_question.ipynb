{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Copy of ps3_student_copy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCq8kJ42ghDl"
      },
      "source": [
        "### TTIC 31020 Introduction to Statistical Machine Learning: SVMs via sub-gradient descent and quadratic programming\n",
        "---\n",
        "**Colab Instructions**\n",
        "\n",
        "You will need access to Google Drive for this; if you don't have it, let us know and we will figure out how to proceed. Also, when you make a copy, it will probably be saved in some default location; you can then move it wherever you like. When you open the notebook, tell Google to open it with Colab.\n",
        "\n",
        "**About Colab**\n",
        "\n",
        "Colab is essentially an online version of Jupyter. The functionality is essentially the same, but we get the added benefit of some nicer UI elements and a remote runtime. In particular, none of this code will run on your local machine (unless you explicitly ask Colab to connect to a local runtime), and all the datasets are hosted remotely.\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "This exact version of the notebook is read-only, so you'll need to copy this into your Drive (File -> Save a copy in Drive) and modify that version.\n",
        "\n",
        "There are two implementation parts to this exercise.\n",
        "\n",
        "**Linear SVM**\n",
        "\n",
        "You will start by implementing a linear SVM using subgradient descent on the primal form. Recall that we can write this as:\n",
        "You will start by implementing a linear SVM using subgradient descent on the primal form. Recall that we can write this as:\n",
        "\\begin{equation*}\n",
        "    \\mathbf{w}^\\ast,b^\\ast = \\arg\\!\\min \\frac{1}{2} \\lVert w \\rVert^2 + C\\sum_{i=1}^N [~ 1 -  y_i(\\mathbf{w} \\cdot \\mathbf{x} +b)~]_+\n",
        "\\end{equation*}\n",
        "Your task here is to implement subgradient descent to calculate an approximation to $\\mathbf{w}^*$.\n",
        "\n",
        "**Kernel SVM**\n",
        "\n",
        "Next, you're going to implement a kernel SVM and test a few kernels on some sanity checks. You will use the _dual_ formulation of the kernel SVM that's in the lecture slides.\n",
        "\n",
        "Once you express the QP for the dual kernel SVM problem in its canonical form, you will plug this into a black-box convex program solver called $\\texttt{cvxopt}$. You will find more specific instructions in the relevant cell.\n",
        "\n",
        "**Evaluation**\n",
        "\n",
        "To check that your code does the right thing, we've included a few synthetic tasks and visualizations. Your code should run pretty fast on these synthetic examples, so you can make changes and iterate quickly.\n",
        "\n",
        "Then, you will test your code on a text classification task; see the relevant cells for more details. **You should get at least 70% test accuracy on the test classification task.**\n",
        "\n",
        "**Exporting for Kaggle**\n",
        "\n",
        "Run the final cell to export the csv file containing your test outputs; then, upload this to Kaggle.\n",
        "\n",
        "**Submission**\n",
        "\n",
        "Please paste a link to your notebook in your assignment and set the sharing settings such that anyone with the link can view your notebook.\n",
        "\n",
        "**Before coding**\n",
        "\n",
        "Before you start writing the code for this assignment, you may find it helpful to first handwrite the gradient descent update step. \n",
        "\n",
        "Similarly, you might find it helpful to first handwrite the QP corresponding to the dual kernel SVM problem before coding this up so that you can keep track of all the variables in the problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNN6eZW5ghDm",
        "cellView": "form"
      },
      "source": [
        "#@title Imports and test code. \n",
        "#@markdown Double click here to see exactly what the code's doing.\n",
        "\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "import csv\n",
        "import cvxopt\n",
        "\n",
        "def create_submission_file(fname, preds):\n",
        "    \"\"\"Create Kaggle submision with predictions written as a csv file\n",
        "    \"\"\"\n",
        "    ofile  = open(fname, \"w\")  \n",
        "    writer = csv.writer(\n",
        "        ofile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL\n",
        "    )\n",
        "    writer.writerow(['id', 'category'])\n",
        "    for i in range(preds.shape[0]):\n",
        "        writer.writerow([i, preds[i]])\n",
        "\n",
        "\n",
        "def plot_decision_countour(svm, X, y, grid_size=100):\n",
        "    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
        "    y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, grid_size),\n",
        "                         np.linspace(y_min, y_max, grid_size),\n",
        "                         indexing='ij')\n",
        "    data = np.stack([xx, yy], axis=2).reshape(-1, 2)\n",
        "    pred = svm.predict(data).reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, pred,\n",
        "                 cmap=cm.Paired,\n",
        "                 levels=[-0.001, 0.001],\n",
        "                 extend='both',\n",
        "                 alpha=0.8)\n",
        "    flatten = lambda m: np.array(m).reshape(-1,)\n",
        "    plt.scatter(flatten(X[:,0][y==-1]),flatten(X[:,1][y==-1]),\n",
        "                  c=flatten(y)[y==-1],cmap=cm.Paired,marker='o')\n",
        "    plt.scatter(flatten(X[:,0][y==1]),flatten(X[:,1][y==1]),\n",
        "                  c=flatten(y)[y==1],cmap=cm.Paired,marker='+')\n",
        "    \n",
        "    plt.xlim(x_min, x_max)\n",
        "    plt.ylim(y_min, y_max)\n",
        "    plt.plot()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_SVM(svm, num_samples=500,linear=False):\n",
        "    \"\"\"test svm\n",
        "    \"\"\"\n",
        "    np.random.seed(783923)\n",
        "\n",
        "    X = npr.random((num_samples, 2)) * 2 - 1\n",
        "    if linear:\n",
        "      y = 2 * (X.sum(axis=1) > 0) - 1.0\n",
        "    else: \n",
        "      y = 2 * ((X ** 2).sum(axis=1) - 0.5 > 0) - 1.0\n",
        "    svm.fit(X,y)\n",
        "    \n",
        "    plot_decision_countour(svm, X, y)\n",
        "\n",
        "    from datetime import datetime\n",
        "    np.random.seed(int(round(datetime.now().timestamp())))\n",
        "\n",
        "def compute_acc(model, X, y):\n",
        "    pred = model.predict(X)\n",
        "    size = len(y)\n",
        "    num_correct = (pred == y).sum()\n",
        "    acc = num_correct / size\n",
        "    print(\"{} out of {} correct, acc {:.3f}\".format(num_correct, size, acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNDW9Ia2ghDq",
        "cellView": "form"
      },
      "source": [
        "#@title Linear SVM class **(To be modified)**\n",
        "#@markdown This class contains a shell of an implementation of a linear SVM.\n",
        "#@markdown Double click to expand and see what needs to be implemented.\n",
        "\n",
        "class LinearSVM():\n",
        "    def __init__(self,C):\n",
        "        \"\"\"initialize the svm\n",
        "        \n",
        "        Args:\n",
        "            C: weight associated with the hinge loss term in the SVM loss\n",
        "        \"\"\"\n",
        "        self.w = None\n",
        "        self.bias = None\n",
        "        self.C = C\n",
        "\n",
        "    def fit(self, X, y,num_epochs=30,lr_sched=lambda t: 0.1/t):\n",
        "        \"\"\"Fit the model on the data\n",
        "        \n",
        "        Args:\n",
        "            X: [N x d] data matrix\n",
        "            y: [N, ] array of labels\n",
        "            num_epochs: number of passes over the training data we make\n",
        "            lr_sched: function determining how the learning rate decays across\n",
        "                      epochs\n",
        "        \n",
        "        Returns:\n",
        "            self, in case you want to build a pipeline\n",
        "        \"\"\"\n",
        "        assert np.ndim(X) == 2, 'data matrix X expected to be 2d'\n",
        "        assert np.ndim(y) == 1, 'labels expected to be 1d'\n",
        "        N, d = X.shape\n",
        "        assert N == y.shape[0], 'expect [N, d] data matrix and [N] labels'\n",
        "\n",
        "        # TODO: implement a subgradient descent\n",
        "        \n",
        "        #Shuffle the matrix\n",
        "        shuffling_matrix = np.random.permutation(N)\n",
        "        Xtr = np.take(X,shuffling_matrix,axis = 0)\n",
        "        ytr = np.take(y,shuffling_matrix,axis=0)\n",
        "\n",
        "        #set constants\n",
        "        tol = 1e-4\n",
        "        lam = 1/self.C\n",
        "        self.w = np.zeros(d)\n",
        "        self.bias = 0\n",
        "        t = 0 #steps\n",
        "        \n",
        "\n",
        "        \n",
        "        for i in range(num_epochs):\n",
        "          for j in range(N):\n",
        "            t += 1\n",
        "            if (1 - ytr[j] * (self.bias + np.dot(self.w,Xtr[j,:])) ) > 0:\n",
        "              grad_w = lam*self.w - ytr[j]*Xtr[j,:]\n",
        "              grad_b = -ytr[j]\n",
        "\n",
        "            else:\n",
        "              grad_w = lam*self.w\n",
        "              grad_b = 0\n",
        "            \n",
        "            old_w = self.w\n",
        "            old_b = self.bias\n",
        "\n",
        "            self.w = old_w - lr_sched(t)*grad_w/0.1/lam\n",
        "            self.bias = old_b - lr_sched(t)*grad_b/0.1/lam\n",
        "\n",
        "            if np.linalg.norm(self.w-old_w) < tol and abs(self.bias - old_b) < tol :\n",
        "              break\n",
        "          \n",
        "          else:\n",
        "            continue\n",
        "          break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        print(\"training complete\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X, binarize=True):\n",
        "        \"\"\"make a prediction and return either the confidence margin or label\n",
        "        \n",
        "        Args:\n",
        "            X: [N, d] array of data or [d,] single data point\n",
        "            binarize: if True, then return the label, else the confidence margin\n",
        "        \n",
        "        Returns:\n",
        "            Either confidence margin or predicted label\n",
        "        \"\"\"\n",
        "        if self.w is None:\n",
        "            raise ValueError(\"go fit the data first\")\n",
        "        X = np.atleast_2d(X)\n",
        "        assert X.shape[1] == self.w.shape[0]\n",
        "        res = X.dot(self.w)\n",
        "        res = res.squeeze()+self.bias\n",
        "        if binarize:\n",
        "            return (res > 0).astype(np.int32) * 2 - 1\n",
        "        else:\n",
        "            return res\n",
        "\n",
        "    def clone(self):\n",
        "        \"\"\"construct a fresh copy of myself\n",
        "        \"\"\"\n",
        "        return LinearSVM(self.lamb, self.num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1cTXCKYghDv"
      },
      "source": [
        "#@title Kernel class. \n",
        "#@markdown This class contains a base implementation of several common\n",
        "#@markdown kernels. If you want to implement custom kernels, you should do that\n",
        "#@markdown here.\n",
        "\n",
        "\n",
        "class Kernel(object):\n",
        "    \"\"\"\n",
        "    A class containing all kinds of kernels.\n",
        "    Note: the kernel should work for both input (Matrix, vector) and (vector, vector)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def linear():\n",
        "        def f(x, y):\n",
        "            return np.dot(x, y)\n",
        "        return f\n",
        "\n",
        "    @staticmethod\n",
        "    def gaussian(sigma):\n",
        "        def f(x, y):\n",
        "            exponent = - (1/sigma**2) * np.linalg.norm((x-y).transpose(), 2, 0) ** 2\n",
        "            return np.exp(exponent)\n",
        "        return f\n",
        "\n",
        "    @staticmethod\n",
        "    def _poly(dimension, offset):\n",
        "        def f(x, y):\n",
        "            return (offset + np.dot(x, y)) ** dimension\n",
        "        return f\n",
        "\n",
        "    @staticmethod\n",
        "    def inhomogenous_polynomial(dimension):\n",
        "        return Kernel._poly(dimension=dimension, offset=1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def homogenous_polynomial(dimension):\n",
        "        return Kernel._poly(dimension=dimension, offset=0.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def hyperbolic_tangent(kappa, c):\n",
        "        def f(x, y):\n",
        "            return np.tanh(kappa * np.dot(x, y) + c)\n",
        "        return f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSlLz8W7ghDw"
      },
      "source": [
        "#@title Kernel SVM class **(To be modified)**\n",
        "#@markdown In this class, you will fill in the missing functions to\n",
        "#@markdown complete the implementation of the Kernel SVM. We will set up the QP\n",
        "#@markdown optimization and plug it into cvxopt, a black-box convex optimization\n",
        "#@markdown library for Python.\n",
        "\n",
        "class KernelSVM(object):\n",
        "    def __init__(self, kernel, C):\n",
        "        \"\"\"\n",
        "        Build a SVM given kernel function and C\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        kernel : function\n",
        "            a function takes input (Matrix, vector) or (vector, vector)\n",
        "        C : a scalar\n",
        "            balance term\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        \"\"\"\n",
        "        self._kernel = kernel\n",
        "        self.C = C\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model given data X and ground truth label y\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2D array\n",
        "            N x d data matrix (row per example)\n",
        "        y : 1D array\n",
        "            class label\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        \"\"\"\n",
        "        # Solve the QP problem to get the multipliers\n",
        "        lagrange_multipliers = self._compute_multipliers(X, y)\n",
        "        print(lagrange_multipliers)\n",
        "        # Get all the support vectors, support weights and bias\n",
        "        self._construct_predictor(X, y, lagrange_multipliers)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the label given data X\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2D array\n",
        "            N x d data matrix (row per example)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y : 1D array\n",
        "            predicted label\n",
        "        \"\"\"\n",
        "        result = np.full(X.shape[0], self._bias) # note: intializing scores with b\n",
        "        for z_i, x_i, y_i in zip(self._weights,\n",
        "                                 self._support_vectors,\n",
        "                                 self._support_vector_labels):\n",
        "            result += z_i * y_i * self._kernel(X, x_i) # the result is \\sum_i alpha_i*y_i*x_i+b\n",
        "        return np.sign(result)\n",
        "\n",
        "    def _kernel_matrix(self, X):\n",
        "        \"\"\"\n",
        "        Get the kernel matrix.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2D array\n",
        "            N x d data matrix (row per example)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        K : 2D array\n",
        "            N x N kernel matrix, where K[i][j] = inner_product(phi(i), phi(j))\n",
        "        \"\"\"\n",
        "        # TODO: implement\n",
        "        \n",
        "        N, d = X.shape\n",
        "\n",
        "        K = np.zeros([N,N])\n",
        "\n",
        "        for i in range(N):\n",
        "          K[i] = self._kernel(X,X[i])\n",
        "        \n",
        "        return K \n",
        "\n",
        "    def _construct_predictor(self, X, y, lagrange_multipliers):\n",
        "        \"\"\"\n",
        "        Given the data, label and the multipliers, extract the support vectors and calculate the bias\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2D array\n",
        "            N x d data matrix (row per example)\n",
        "        y : 1D array\n",
        "            class label\n",
        "        lagrange_multipliers: 1D array\n",
        "            the solution of lagrange_multiplier\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        \"\"\"\n",
        "        support_vector_indices = \\\n",
        "            lagrange_multipliers > 1e-5\n",
        "            \n",
        "        print(\"SV number: \", np.sum(support_vector_indices))\n",
        "\n",
        "        support_multipliers = lagrange_multipliers[support_vector_indices]\n",
        "        support_vectors = X[support_vector_indices]\n",
        "        support_vector_labels = y[support_vector_indices]\n",
        "\n",
        "        \"\"\"\n",
        "        Get the bias term\n",
        "        \"\"\"\n",
        "        # TODO: implement\n",
        "\n",
        "        positive_vector_indices = support_vector_labels > 0\n",
        "        positive_vectors = support_vectors[positive_vector_indices]\n",
        "        negative_vectors = support_vectors[~positive_vector_indices]\n",
        "\n",
        "        positive_results = np.zeros(positive_vectors.shape[0])\n",
        "        negative_results = np.zeros(negative_vectors.shape[0])\n",
        "\n",
        "        for i in range(positive_results.shape[0]):\n",
        "          positive_results[i] = np.dot(support_multipliers* \\\n",
        "                                       support_vector_labels, \\\n",
        "                                self._kernel(support_vectors,\\\n",
        "                                        positive_vectors[i]) \\\n",
        "                                        )\n",
        "          \n",
        "        for j in range(negative_results.shape[0]):\n",
        "          negative_results[j] = np.dot(support_multipliers* \\\n",
        "                                       support_vector_labels, \\\n",
        "                                self._kernel(support_vectors,\\\n",
        "                                        negative_vectors[j]) \\\n",
        "                                        )\n",
        "          \n",
        "        print(positive_results)\n",
        "        print(negative_results)\n",
        "        bias =  - ( np.amax(negative_results) + \\\n",
        "                   np.amin(positive_results) )/2\n",
        "\n",
        "        self._bias=bias\n",
        "        self._weights=support_multipliers\n",
        "        self._support_vectors=support_vectors\n",
        "        self._support_vector_labels=support_vector_labels\n",
        "\n",
        "\n",
        "    def _compute_multipliers(self, X, y):\n",
        "        \"\"\"\n",
        "        Given the data, label, solve the QP program to get lagrange multiplier.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2D array\n",
        "            N x d data matrix (row per example)\n",
        "        y : 1D array\n",
        "            class label\n",
        "\n",
        "        Returns\n",
        "        lagrange_multipliers: 1D array\n",
        "        -------\n",
        "        \"\"\"\n",
        "        N, d = X.shape\n",
        "\n",
        "        K = self._kernel_matrix(X)\n",
        "        \"\"\"\n",
        "        The standard QP solver formulation:\n",
        "        min 1/2 alpha^T H alpha + f^T alpha\n",
        "        s.t.\n",
        "        A * alpha \\coneleq a (A is former G)\n",
        "        B * alpha = b\n",
        "        \"\"\"\n",
        "        # TODO: implement. Specifically, define the H, f, A, a, B, b arguments\n",
        "        # as indicated above.\n",
        "\n",
        "        f = cvxopt.matrix(np.ones(N))\n",
        "        \n",
        "        y_re = np.reshape(y,[-1,1])\n",
        "        H = cvxopt.matrix( np.dot(y_re,y_re.T)*K )\n",
        "\n",
        "        A = cvxopt.matrix( np.vstack( [np.eye(N),-np.eye(N)] ))\n",
        "\n",
        "        a = cvxopt.matrix( np.hstack( [self.C*np.ones(N) ,\\\n",
        "                                       np.zeros(N)] ) )\n",
        "\n",
        "        B = cvxopt.matrix( np.reshape(y,[1,-1]) )\n",
        "\n",
        "        b = cvxopt.matrix( np.zeros(1) )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        solution = cvxopt.solvers.qp(H, f, A, a, B, b)\n",
        "\n",
        "        # Lagrange multipliers\n",
        "        return np.ravel(solution['x'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKPHo4ADghDs",
        "cellView": "form",
        "outputId": "37c10dd8-5460-489e-c0fc-2ece9691221d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "#@title Linear SVM Sanity Check\n",
        "\n",
        "svm = LinearSVM(C=10)\n",
        "test_SVM(svm,linear=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-n9yvxD_Px",
        "outputId": "9975e3fe-723d-4bc8-d544-1a30d4fcde34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "svm = KernelSVM(Kernel.linear(), C=50)\n",
        "av = np.array([[1,2,3],[4,5,6],[7,8,9],[-1,2,3],[4,-5,6]])\n",
        "svm._kernel_matrix(av)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 14.,  32.,  50.,  12.,  12.],\n",
              "       [ 32.,  77., 122.,  24.,  27.],\n",
              "       [ 50., 122., 194.,  36.,  42.],\n",
              "       [ 12.,  24.,  36.,  14.,   4.],\n",
              "       [ 12.,  27.,  42.,   4.,  77.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwABjvmx5YU0",
        "outputId": "f3c52a38-cfde-4a00-fc13-43d6947aaa22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title Kernel (Linear) SVM Sanity Check\n",
        "\n",
        "svm = KernelSVM(Kernel.linear(), C=50)\n",
        "test_SVM(svm,linear=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     pcost       dcost       gap    pres   dres\n",
            " 0:  6.6086e+03 -7.9551e+05  2e+06  7e-01  1e-13\n",
            " 1:  1.0373e+04 -2.0110e+05  3e+05  7e-02  1e-13\n",
            " 2:  8.6993e+03 -2.5772e+04  4e+04  9e-03  2e-13\n",
            " 3:  2.6619e+03 -4.6635e+03  8e+03  9e-04  7e-14\n",
            " 4:  7.4102e+02 -1.3621e+03  2e+03  1e-04  2e-14\n",
            " 5:  1.2965e+02 -1.4887e+02  3e+02  9e-15  1e-14\n",
            " 6:  2.0942e+01 -1.2472e+01  3e+01  8e-16  4e-15\n",
            " 7:  2.5950e+00 -6.5296e-01  3e+00  2e-16  2e-15\n",
            " 8:  1.9287e-01 -1.4518e-02  2e-01  2e-16  7e-16\n",
            " 9:  3.5287e-03 -9.3661e-05  4e-03  2e-16  7e-16\n",
            "10:  3.5286e-05 -9.0176e-07  4e-05  2e-16  5e-16\n",
            "11:  3.5285e-07 -9.0141e-09  4e-07  2e-16  6e-16\n",
            "12:  3.5285e-09 -9.0141e-11  4e-09  2e-16  5e-16\n",
            "Optimal solution found.\n",
            "[3.73517804e-12 9.08319958e-12 2.27388820e-12 4.10768595e-13\n",
            " 8.08554298e-13 1.62209508e-11 3.15881711e-12 8.00976755e-12\n",
            " 1.94873511e-13 4.14709645e-13 6.48638172e-12 7.61641371e-12\n",
            " 4.33469427e-12 7.28659817e-12 4.22387713e-13 7.43825108e-12\n",
            " 7.53244039e-12 7.10832255e-13 1.39359311e-11 1.49268606e-11\n",
            " 9.92650465e-12 3.66112092e-12 5.74987848e-12 7.22322755e-12\n",
            " 4.32145533e-12 1.84171191e-11 1.41582608e-12 5.75050689e-12\n",
            " 1.63346816e-12 7.18246344e-13 4.15259497e-12 1.93689627e-13\n",
            " 7.58439233e-12 7.77072667e-12 3.00968641e-13 2.33352691e-11\n",
            " 8.71733527e-13 1.93823538e-11 8.62279454e-12 7.11826084e-12\n",
            " 7.83871024e-12 1.08087803e-12 6.38527712e-13 1.10914645e-12\n",
            " 2.87511688e-13 3.93894021e-12 1.76703658e-11 3.62443607e-11\n",
            " 8.14860179e-12 6.79880777e-12 3.60855440e-12 8.04679830e-12\n",
            " 2.08465751e-13 6.54333006e-12 6.91747209e-12 7.49398268e-12\n",
            " 1.94800039e-13 3.64908788e-12 7.31160597e-12 6.05905808e-12\n",
            " 3.32190505e-12 4.31160284e-12 1.85373183e-13 2.41746472e-12\n",
            " 1.48854043e-12 2.57926807e-12 2.97554512e-12 4.20777620e-12\n",
            " 6.46704419e-12 7.04840479e-12 2.88003389e-11 3.19070621e-11\n",
            " 7.84765355e-12 2.00468824e-12 1.83813133e-11 6.14111857e-13\n",
            " 1.96310408e-13 1.81489928e-13 1.40376483e-11 1.48112687e-11\n",
            " 3.79926474e-13 6.94357391e-12 1.75333744e-11 2.23517443e-13\n",
            " 7.45386119e-12 2.73789128e-12 1.32453988e-12 8.39000901e-12\n",
            " 6.33935167e-12 8.27625089e-12 1.73142196e-12 6.95937272e-13\n",
            " 6.92982341e-12 1.26854727e-11 6.84474447e-12 7.79878513e-12\n",
            " 7.47434571e-12 1.68273738e-12 3.26110931e-12 7.50161231e-12\n",
            " 5.73775858e-13 4.54806178e-12 1.17722560e-12 1.59941722e-11\n",
            " 1.01634803e-11 7.05947648e-12 3.25425842e-11 2.61996413e-12\n",
            " 8.94959112e-12 8.47808493e-12 7.70827043e-12 1.20843023e-12\n",
            " 6.91917623e-12 2.52890149e-13 2.39632227e-11 3.56012461e-12\n",
            " 3.29850531e-11 1.17464378e-11 7.71200327e-12 5.56550094e-13\n",
            " 7.38432772e-12 1.38632366e-11 6.41484703e-12 7.99338939e-12\n",
            " 7.62388330e-12 7.12992624e-12 1.86174412e-11 7.39141227e-12\n",
            " 5.92792978e-12 7.88419407e-12 2.58747115e-12 1.85104201e-11\n",
            " 1.77856151e-13 1.21948903e-11 7.12464567e-12 8.51122785e-12\n",
            " 6.71639416e-12 1.12312073e-12 1.70997806e-12 7.03272727e-12\n",
            " 1.50697501e-11 1.64012026e-11 1.38958067e-11 5.99985178e-12\n",
            " 9.38076295e-12 7.11573368e-12 1.42838439e-11 1.21158919e-11\n",
            " 1.92380023e-11 7.18071104e-12 2.82170489e-12 2.04816745e-13\n",
            " 4.38359559e-13 4.81888726e-13 3.59841696e-12 1.85495500e-12\n",
            " 7.02516437e-12 3.92667347e-13 2.10166389e-12 1.80265655e-13\n",
            " 2.97829220e-12 8.48373522e-12 7.40171200e-12 2.09795828e-13\n",
            " 9.00507255e-13 5.07361732e-12 1.77240923e-12 7.61790336e-12\n",
            " 9.83158406e-12 7.16887130e-12 5.64289563e-12 8.37858866e-12\n",
            " 1.17685577e-11 7.31734949e-12 4.97536934e-12 8.12880023e-13\n",
            " 1.58902220e-12 8.74762761e-12 8.79037343e-12 3.35136822e-12\n",
            " 8.00257308e-12 1.05776693e-11 6.68635359e-12 7.89620664e-12\n",
            " 1.04916813e-11 3.97365787e-12 8.39174071e-12 7.31413960e-12\n",
            " 8.40597427e-12 1.91188204e-11 8.93599299e-13 7.17868606e-12\n",
            " 6.70985911e-12 7.45928585e-13 2.65621143e-13 1.32287464e-11\n",
            " 7.99724355e-12 7.31679752e-12 2.29810871e-12 7.26122280e-12\n",
            " 7.16839855e-12 2.40566248e-12 8.10957136e-12 2.97180243e-13\n",
            " 9.91224360e-12 2.23406691e-13 1.36627074e-12 3.77450406e-12\n",
            " 1.10248686e-11 2.68445927e-13 2.84871224e-11 2.25871663e-12\n",
            " 6.26710771e-12 3.13213741e-12 6.32263916e-12 8.28038813e-12\n",
            " 3.31578962e-13 7.48130563e-12 9.00536718e-12 7.39030980e-12\n",
            " 6.15303969e-13 1.17828839e-11 2.77635608e-12 6.18365009e-12\n",
            " 8.10597911e-12 7.99881606e-12 1.89377111e-11 6.93595513e-12\n",
            " 8.11671566e-12 3.00303198e-13 7.04692932e-12 8.66219345e-12\n",
            " 1.87068384e-13 1.72624601e-12 3.47492927e-13 9.02884921e-13\n",
            " 4.20909746e-12 2.33101823e-12 7.54119750e-12 4.22225430e-12\n",
            " 1.75589808e-11 7.45832478e-12 8.06207002e-12 8.59027227e-12\n",
            " 8.02246849e-12 7.86379037e-12 7.83914361e-12 2.40857771e-13\n",
            " 2.13642225e-12 4.53663309e-13 4.43239544e-12 7.20804908e-12\n",
            " 8.16366352e-12 4.73026738e-12 2.83983610e-11 1.05528036e-12\n",
            " 5.63429136e-12 1.94283500e-13 5.70221559e-12 8.05346679e-12\n",
            " 3.80793744e-12 5.71263118e-13 6.98251473e-12 4.97416738e-12\n",
            " 4.19742033e-12 1.58159539e-12 1.99767597e-13 1.27981539e-12\n",
            " 1.09297077e-12 1.53081425e-11 6.37495600e-12 3.17766943e-12\n",
            " 8.03936181e-12 9.70712299e-12 3.38434776e-11 4.59317314e-12\n",
            " 8.52833889e-12 3.20001519e-12 8.60819584e-12 7.34307980e-12\n",
            " 2.62055884e-12 5.96299484e-13 7.04523499e-13 7.03264110e-12\n",
            " 8.50034160e-12 7.05821676e-12 8.28582867e-12 8.18386468e-12\n",
            " 6.52919428e-12 2.69573030e-12 1.46564987e-11 9.77071730e-13\n",
            " 5.17571946e-12 1.47083931e-11 1.12504493e-11 2.16227218e-13\n",
            " 7.88293308e-12 2.11009980e-11 7.55527773e-12 3.45702756e-13\n",
            " 4.23874593e-13 5.19609215e-12 7.85016647e-13 7.07877064e-12\n",
            " 7.60505941e-12 2.49126444e-11 8.12023916e-12 6.81393213e-12\n",
            " 4.46368142e-13 8.27998467e-12 8.19745242e-12 8.13169060e-12\n",
            " 5.01500381e-13 2.59939546e-13 4.67765596e-12 5.15314504e-12\n",
            " 5.33097265e-13 2.19776962e-12 1.83408437e-13 7.77827576e-12\n",
            " 9.37383903e-12 2.96166205e-12 3.74660209e-12 6.83108263e-12\n",
            " 9.52356167e-13 7.89235260e-12 3.88696750e-12 7.30412748e-12\n",
            " 8.52172200e-12 5.27207891e-12 8.11725958e-12 7.14241349e-12\n",
            " 2.20805750e-13 1.08637064e-11 2.85766132e-12 7.43888161e-12\n",
            " 7.39369908e-12 1.54144887e-12 4.16348200e-13 2.16348412e-11\n",
            " 8.23497365e-12 3.50473354e-12 7.78815755e-12 4.21785211e-12\n",
            " 7.46210629e-12 7.29233427e-12 1.82766343e-13 3.04074153e-11\n",
            " 2.29516567e-11 7.44303711e-12 5.60282297e-12 6.96604848e-12\n",
            " 2.06793544e-12 7.33212525e-12 6.89367212e-12 2.26566405e-11\n",
            " 3.22921090e-13 6.99014629e-12 8.18223401e-12 1.65687303e-11\n",
            " 6.70480581e-13 8.31102043e-12 1.33924076e-11 7.05247665e-12\n",
            " 4.41039260e-12 1.07867113e-12 3.24412094e-11 1.68082207e-12\n",
            " 1.32919367e-12 1.62530896e-12 1.48552546e-12 7.48495346e-12\n",
            " 5.83522616e-12 2.10582878e-11 2.77583684e-13 3.17824382e-12\n",
            " 7.26234762e-12 1.18132197e-11 6.25942878e-12 2.95098463e-13\n",
            " 1.01037098e-11 7.06216615e-12 7.51460305e-12 7.00346578e-12\n",
            " 7.98789306e-12 1.95070963e-11 5.05917215e-12 3.39009919e-13\n",
            " 7.40240887e-12 7.19253592e-12 3.21119286e-11 7.22896655e-12\n",
            " 1.48593764e-12 6.94430549e-12 6.86371961e-12 7.93454534e-12\n",
            " 1.65814892e-12 1.43283369e-12 2.88392174e-12 1.40351756e-12\n",
            " 8.07636109e-12 5.44897535e-12 4.49684357e-12 2.79438864e-11\n",
            " 7.86346266e-12 8.42439364e-12 2.16458403e-13 1.98459182e-11\n",
            " 3.73986815e-11 3.19862846e-12 2.04512634e-13 2.95702822e-12\n",
            " 1.83446695e-11 7.48241390e-12 7.95627795e-12 2.32977996e-12\n",
            " 2.01006302e-11 7.60213024e-12 7.79827774e-12 7.58715593e-12\n",
            " 3.87891604e-12 1.86715685e-11 8.17082116e-12 1.58974389e-11\n",
            " 3.23965163e-12 2.97732442e-12 7.88650377e-12 1.67524430e-11\n",
            " 1.02302688e-11 9.38751495e-12 1.03113942e-11 1.88929350e-13\n",
            " 9.95145956e-13 9.93584929e-12 8.31211216e-12 7.85313300e-12\n",
            " 6.23338738e-12 6.28561711e-13 7.98686119e-12 6.11285494e-12\n",
            " 8.30281542e-12 8.47954894e-12 6.22022137e-13 3.28282318e-13\n",
            " 8.31776722e-12 2.01725666e-13 4.25417760e-12 7.72215171e-12\n",
            " 2.83068437e-11 2.36014492e-11 2.84737620e-12 4.34583914e-13\n",
            " 7.06119583e-12 3.29438337e-12 3.05728301e-12 8.04554993e-13\n",
            " 5.35289291e-12 1.26414939e-12 2.45373968e-12 2.08007538e-13\n",
            " 7.21591387e-12 7.78670034e-12 6.33962817e-12 8.40109136e-12\n",
            " 7.54210807e-12 6.72358497e-12 8.44837012e-12 6.56492011e-13\n",
            " 8.48830726e-12 7.83414638e-12 8.52821143e-12 2.30233442e-13\n",
            " 7.32900120e-12 2.13140267e-13 1.01590179e-11 3.06101888e-12\n",
            " 7.03940991e-12 8.45183171e-12 1.16363757e-11 7.25234894e-12\n",
            " 6.11441563e-12 1.07729151e-11 1.37315424e-12 1.40724232e-11\n",
            " 1.16564818e-12 7.18546750e-12 9.73378197e-12 2.31183889e-11\n",
            " 7.61537742e-12 8.43959545e-12 6.48477616e-12 8.94611030e-12\n",
            " 8.29356406e-12 8.36923996e-12 3.75020541e-13 2.32951849e-12\n",
            " 7.95092797e-12 7.88325085e-12 2.37645996e-11 9.93923016e-12]\n",
            "SV number:  0\n",
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-e5310792ae36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_SVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-ad5f09dfa36a>\u001b[0m in \u001b[0;36mtest_SVM\u001b[0;34m(svm, num_samples, linear)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mplot_decision_countour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-be83b3e01235>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlagrange_multipliers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Get all the support vectors, support weights and bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlagrange_multipliers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-be83b3e01235>\u001b[0m in \u001b[0;36m_construct_predictor\u001b[0;34m(self, X, y, lagrange_multipliers)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_results\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m                    \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_results\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2666\u001b[0m     \"\"\"\n\u001b[1;32m   2667\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0;32m-> 2668\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOt3BNrrghDy",
        "cellView": "form"
      },
      "source": [
        "#@title Kernel (RBF) SVM Sanity Check\n",
        "#feel free to play with sigma and C\n",
        "svm = KernelSVM(Kernel.gaussian(sigma=0.5), C=10)\n",
        "test_SVM(svm,linear=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugL7eaqIJ6Tc"
      },
      "source": [
        "In the tests above, you should have obtained images that look roughly like this:\n",
        "\n",
        "\n",
        "Linear SVM\n",
        "\n",
        "![linear SVM result](https://drive.google.com/uc?id=1XaRAcsIY1UoRq8xnLY45u01CkzGLAKuB)\n",
        "\n",
        "Linear kernel SVM\n",
        "\n",
        "![linear kernel result](https://drive.google.com/uc?id=1OsuPOEMjbap5M7O2fTWymNwJu7DD9fJa) \n",
        "\n",
        "RBF kernel SVM\n",
        "\n",
        "![RBF kernel result](https://drive.google.com/uc?id=1tbH9STSe7p3dOxt6niJhYI17_Id7HGBY)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvFRZS1SghD1"
      },
      "source": [
        "#### Now that our SVM is working on the toy dataset, let's do a simple sentiment analysis on [tweets on US airline service quality](https://www.kaggle.com/crowdflower/twitter-airline-sentiment/version/2). (WARNING: expletives unfiltered)\n",
        "---\n",
        "- As shown below, our data comes in the form of a csv table. The columns most relevant to our task are 'text' and 'airline_sentiment'.\n",
        "- Data must be represented as a [N x d] matrix, but what we have on our hands is unstructured text.\n",
        "- The simplest solution to transform an airline review into a vector is [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model). We maintain a global vocabulary of word patterns gathered from our corpus, with single words such as \"great\", \"horrible\", and optionally consecutive words (N-grams) like \"friendly service\", \"luggage lost\". Suppose we have already collected a total of 10000 such patterns, to transform a sentence into a 10000-dimensional vector, we simply scan it and look for the patterns that appear and set their correponding entries to 1 and leave the rest at 0. What we end up with is a sparse vector that can be fed into SVMs.\n",
        "- For this exercise we use the basic text processing routines in nltk and sklearn\n",
        "- Our data is not balanced, with siginificant more negatives than neutral + positives. Therefore we group neutral and positive into one category and the final ratio of non-negative vs negative is about 1:2. This is consistent across train, val and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUmn6ej2ghD2"
      },
      "source": [
        "#@markdown Here, we'll set up the dataset. To show you that this is doing\n",
        "#@markdown something useful, we'll print the first three entries of the\n",
        "#@markdown training set.\n",
        "\n",
        "import os.path as osp\n",
        "import pandas as pd\n",
        "import re, nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "data_root = 'data/tweets'\n",
        "!curl -O https://ttic.uchicago.edu/~nsm/ttic_31020_2020/hw_3/dataset/train.csv\n",
        "!curl -O https://ttic.uchicago.edu/~nsm/ttic_31020_2020/hw_3/dataset/val.csv\n",
        "!curl -O https://ttic.uchicago.edu/~nsm/ttic_31020_2020/hw_3/dataset/test_release.csv\n",
        "\n",
        "data_root = ''\n",
        "train, val, test = \\\n",
        "    pd.read_csv(osp.join(data_root, 'train.csv')), \\\n",
        "    pd.read_csv(osp.join(data_root, 'val.csv')), \\\n",
        "    pd.read_csv(osp.join(data_root, 'test_release.csv'))\n",
        "\n",
        "print(train.head(3))\n",
        "\n",
        "print(train.airline_sentiment.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Zp-mi8ghEB",
        "cellView": "form"
      },
      "source": [
        "#@title Vectorizing the vocabulary\n",
        "#@markdown Here, we build the vocabulary and vector representations for each\n",
        "#@markdown word. You may find it useful to modify parts of this later, but it's\n",
        "#@markdown not strictly necessary. We then sample some of the learned\n",
        "#@markdown vocabulary.\n",
        "\n",
        "# check these out \n",
        "stop_words = set(stopwords.words('english'))\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "def tokenize_normalize(tweet):\n",
        "    only_letters = re.sub(\"[^a-zA-Z]\", \" \", tweet)\n",
        "    tokens = nltk.word_tokenize(only_letters)[2:]\n",
        "    lower_case = [l.lower() for l in tokens]\n",
        "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
        "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
        "    return lemmas\n",
        "\n",
        "# the sklearn vectorizer scans our corpus, build the vocabulary, and changes text into vectors\n",
        "vectorizer = CountVectorizer(\n",
        "    strip_accents='unicode', \n",
        "    lowercase=True, \n",
        "    tokenizer=tokenize_normalize,\n",
        "    ngram_range=(1,1),  # you may want to try 2 grams. The vocab will get very large though,\n",
        "    min_df=100,  # this parameter deletes words that occur in less than min_df\n",
        "                # documents. decreasing this will increase the vocabulary size,\n",
        "                # but may also increase the runtime.\n",
        ")\n",
        "# first learn the vocabulary\n",
        "vectorizer.fit(pd.concat([train, val]).text)\n",
        "\n",
        "\n",
        "print( list(vectorizer.vocabulary_.items())[:10] )\n",
        "print(\"\\n vocabulary size {}\".format(len(vectorizer.vocabulary_)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElkjTodLghEG",
        "cellView": "form"
      },
      "source": [
        "#@title Setting up the training set\n",
        "#@markdown Now, we will prepare the training data so that we can call our\n",
        "#@markdown SVM as a black-box.\n",
        "\n",
        "X = {}\n",
        "y = {}\n",
        "X['train'] = vectorizer.transform(train.text).toarray()\n",
        "X['val'] = vectorizer.transform(val.text).toarray()\n",
        "X['test'] = vectorizer.transform(test.text).toarray()\n",
        "\n",
        "# note that our data is 10250 dimensional. \n",
        "# This is a little daunting for laptops and coming up with a manageable vector\n",
        "# representation is a major topic in Natural Language Processing.\n",
        "print(X['train'].shape)\n",
        "\n",
        "# convert the word labels of 'positive', 'neutral', 'negative' into integer labels\n",
        "# note that positive and neural belong to one category, labelled as 1, while negative stands alone as the other\n",
        "for name, dataframe in zip(['train', 'val'], [train, val]):\n",
        "    sentiments_in_words = dataframe['airline_sentiment'].tolist()\n",
        "    int_lbls = np.array( list(map(lambda x: -1 if x == 'negative' else 1, sentiments_in_words)), dtype=np.int32 )\n",
        "    y[name] = int_lbls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "536A07IYghEJ",
        "cellView": "form"
      },
      "source": [
        "#@title Linear SVM on the airline dataset\n",
        "\n",
        "svm = LinearSVM(C=1000)\n",
        "svm.fit(X['train'], y['train'],lr_sched=lambda t: 1/(.1*t), num_epochs=10)\n",
        "compute_acc(svm, X['train'], y['train'])\n",
        "compute_acc(svm, X['val'], y['val'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1VkbuDr5ePe",
        "cellView": "form"
      },
      "source": [
        "#@title Kernel SVM with linear kernel on the airline dataset (optional)\n",
        "#@markdown This could take a while to run.\n",
        "\n",
        "print(X['train'].shape)\n",
        "\n",
        "svm = KernelSVM(Kernel.linear(), C=100)\n",
        "svm.fit(X['train'].astype(float), y['train'].astype(float))\n",
        "compute_acc(svm, X['train'], y['train'])\n",
        "compute_acc(svm, X['val'], y['val'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2u8o5Y5HAr6",
        "cellView": "form"
      },
      "source": [
        "#@title Kernel SVM with RBF kernel on the airline dataset (optional)\n",
        "#@markdown This could take a while to run.\n",
        "\n",
        "print(X['train'].shape)\n",
        "\n",
        "svm = KernelSVM(Kernel.gaussian(sigma=1), C=10)\n",
        "svm.fit(X['train'].astype(float), y['train'].astype(float))\n",
        "compute_acc(svm, X['train'], y['train'])\n",
        "compute_acc(svm, X['val'], y['val'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e9OUfEQghEO"
      },
      "source": [
        "- Try to come up with a better text feature representation. We threw out all the emojis. >< what a waste\n",
        "- Play around with different parameter settings and upload your test set predictions to kaggle.\n",
        "- Given the high feature dimensionality of our primitive text processing, we do not recommend using kernel SVM here. It could take a long time to train. If you reduce the feature dimensionality, then it's a different story"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxC9ZhkIghER",
        "cellView": "form"
      },
      "source": [
        "#@title Generate test output\n",
        "#@markdown Run this cell to generate the csv containing your test outputs for\n",
        "#@markdown Kaggle.\n",
        "#@markdown Contest link: https://www.kaggle.com/c/ttic31020hw3\n",
        "\n",
        "test_pred = svm.predict(X['test'])\n",
        "create_submission_file('tweet_test.csv', test_pred)\n",
        "from google.colab import files\n",
        "files.download('tweet_test.csv') "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}